import csv
import re  # ì¶”ê°€: ì •ê·œí‘œí˜„ì‹ ëª¨ë“ˆ
from konlpy.tag import Okt
import pandas as pd
from mysql.connector import pooling

word_to_index = {}
bow = []


pool=pooling.MySQLConnectionPool(pool_name="pynative_pool",
                                pool_size=32,
                                pool_reset_session=True,
                                host="ls-8afd880ef3938cc6bff7db1f49dfcbcde8311b12.crqrymk8yrjv.ap-northeast-2.rds.amazonaws.com",
                                user="coalaroot",
                                password="coaladbpass",
                                database="CoalaService",
                                charset="utf8"
                                )
def get_connection():
    return pool.get_connection()


def convert_synonym_pattern(syn):
    syn = syn.replace("%d", r"\d+")
    syn = syn.replace("%s", r"\S+")
    syn = syn.replace("%c", r"\S{1}")
    syn = syn.replace("%Text", r"\b[a-zA-Z]{2,}+")
    syn = syn.replace(" ", r"[\s\S]{0,10}?")
    return syn


def kw_synonym_check(keyword_list, synonym_dict, ai_lost_key, content, ai_response):
    for keyword in keyword_list:
        synonyms = synonym_dict.get(keyword, [])  # ë³€í™˜ëœ íŒ¨í„´ ìƒì„±
        converted_patterns = [convert_synonym_pattern(syn) for syn in synonyms]
        pattern = "|".join(converted_patterns)

        has_synonym = bool(re.search(pattern, content))

        if keyword not in ai_lost_key and not has_synonym:
            ai_lost_key.append(keyword)

        elif keyword in ai_lost_key and has_synonym:
            ai_lost_key.remove(keyword)
            if keyword in ai_response["idea_error_object"]:
                idx = ai_response["idea_error_object"].index(keyword)
                if ai_response["idea_error_reason"][idx] == "ëˆ„ë½":
                    del ai_response["non_passed_standard"][idx]
                    del ai_response["idea_error_object"][idx]
                    del ai_response["idea_error_reason"][idx]
                    del ai_response["idea_error_detail"][idx]
                    del ai_response["idea_error_advice"][idx]

    if "" in ai_lost_key:
        ai_lost_key.remove("")

    if not ai_lost_key:
        keyword_part = ""
    elif len(ai_lost_key) == 1:
        keyword_part = ai_lost_key[0]
    else:
        keyword_part = ", ".join(ai_lost_key)
    print(keyword_part)
    return ai_response, keyword_part


def build_bag_of_words(morphs):
    """BoW ìƒì„±"""
    for word in morphs:  
        if word not in word_to_index.keys():
            word_to_index[word] = len(word_to_index)  
            # ìˆ˜ì •: insert ëŒ€ì‹  append ì‚¬ìš©
            bow.append(1)
        else:
            # ì¬ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ ì¸ë±ìŠ¤
            index = word_to_index.get(word)
            # ì¬ë“±ì¥í•œ ë‹¨ì–´ëŠ” í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ì˜ ìœ„ì¹˜ì— 1ì„ ë”í•œë‹¤.
            bow[index] = bow[index] + 1
    return word_to_index, bow

def open_csv(csv_path):
    """CSV ì—´ê¸°"""
    with open(csv_path, newline='', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        data = [row for row in reader]
    return data

def make_idea_morphs(data, stop_words_set):
    """ì•„ì´ë””ì–´ í˜•íƒœì†Œ ë¶„ì„"""
    okt = Okt()

    for row in data:
        idea_text = row['idea']
        morphs = okt.morphs(idea_text)
        # ìˆ˜ì •: ë¦¬ìŠ¤íŠ¸ ìˆœíšŒ ì¤‘ ìˆ˜ì • ëŒ€ì‹  í•„í„°ë§ìœ¼ë¡œ ë³€ê²½
        filtered_morphs = [morph for morph in morphs if morph not in stop_words_set]
        row['idea_morphs'] = filtered_morphs  # ìˆ˜ì •: ì»¬ëŸ¼ëª… ì˜¤íƒ€ ìˆ˜ì •
    return data

def load_stopwords():
    # íŒŒì¼ ê²½ë¡œë¥¼ ë¬¸ìì—´ë¡œ ì „ë‹¬ (ì‰¼í‘œ ì œê±°)
    stopwords_path = r'c:\Users\jeayy\Desktop\NLP\find_idea_similarity\DTM(Document-Term Matrix)\stopwords-ko.txt'
    with open(stopwords_path, encoding='utf-8') as f:
        stop_words_list = [line.strip() for line in f if line.strip()]
    stop_words_list.extend(['.','ì•„ì´ë””ì–´',','])
    stop_words_set = set(stop_words_list)
    return stop_words_set
    
def DTM(csv_path, stopwords_set):
    """DTM ìƒì„±"""
    with open(csv_path, newline='', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        data = [row for row in reader]

    okt = Okt()

    for row in data:
        idea_text = row['idea']
        morphs = okt.morphs(idea_text)
        # ìˆ˜ì •: ë¦¬ìŠ¤íŠ¸ ìˆœíšŒ ì¤‘ ìˆ˜ì • ëŒ€ì‹  í•„í„°ë§ìœ¼ë¡œ ë³€ê²½
        filtered_morphs = [morph for morph in morphs if morph not in stopwords_set]
        row['idea_morphs'] = filtered_morphs  # ìˆ˜ì •: ì»¬ëŸ¼ëª… ì˜¤íƒ€ ìˆ˜ì •
        build_bag_of_words(filtered_morphs)
        
    print(word_to_index)
    print(bow)

    word_df = pd.DataFrame({
        'word': list(word_to_index.keys()),
        'index': list(word_to_index.values()),
        'count': bow
    })

    # ì €ì¥
    word_df.to_csv(r'c:\Users\jeayy\Desktop\NLP\find_idea_similarity\DTM(Document-Term Matrix)\30013_word_frequency_table.csv', index=False, encoding='utf-8-sig')

    return word_df

def check_essential_words(idea_text, essential_words):
    """í•„ìˆ˜ ë‹¨ì–´ í™•ì¸ í•¨ìˆ˜"""
    found_categories = []
    missing_categories = []
    
    for category, synonyms in essential_words.items():
        found = False
        for synonym in synonyms:
            if synonym in idea_text:
                found = True
                break
        
        if found:
            found_categories.append(category)
        else:
            missing_categories.append(category)
    
    return found_categories, missing_categories

def find_over_threshold_words(word_df, idea_df_dict, essential_words, score_threshold=4, frequency_threshold=20):
    """íŠ¹ì • ì ìˆ˜ ì´ìƒì˜ ë‹¨ì–´ ì°¾ê³  í•„ìˆ˜ ë‹¨ì–´ í™•ì¸"""
    filtered_ideas = []
    qualified_ideas = []  # í•„ìˆ˜ ë‹¨ì–´ë¥¼ ëª¨ë‘ í¬í•¨í•œ ì•„ì´ë””ì–´ë“¤
    
    for idx, row in idea_df_dict.iterrows():
        score = 0
        for morph in row['idea_morphs']:
            if morph in word_df['word'].values:
                word_index = word_df[word_df['word'] == morph].index[0]
                if word_df.at[word_index, 'count'] > frequency_threshold:
                    score += 1
        
        if score > score_threshold:
            idea_text = row['idea']
            found_categories, missing_categories = check_essential_words(idea_text, essential_words)
            
            result = {
                'idea': idea_text,
                'score': score,
                'found_categories': found_categories,
                'missing_categories': missing_categories,
                'has_all_essential': len(missing_categories) == 0
            }
            
            filtered_ideas.append(result)
            
            print(f" score = {score}, ì•„ì´ë””ì–´: {idea_text}")
            print(f" í¬í•¨ëœ í•„ìˆ˜ ë‹¨ì–´: {found_categories}")
            print(f" ëˆ„ë½ëœ í•„ìˆ˜ ë‹¨ì–´: {missing_categories}")
            
            # ëª¨ë“  í•„ìˆ˜ ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ìš°
            if len(missing_categories) == 0:
                qualified_ideas.append(result)
                print(" âœ… ëª¨ë“  í•„ìˆ˜ ë‹¨ì–´ í¬í•¨!")
            print("-" * 50)
    
    return filtered_ideas, qualified_ideas

essential_words = {
    '1':["1","í•˜ë‚˜","ì¼","í•œ"],
    '10':["10","ì—´","ì‹­"],
    'ë”í•˜ê¸°': ["ë”í•˜ê¸°","ë”í•œ","ë”í•˜","+","í•©","ë§ì…ˆ","ë”í•´"],
    'ëˆ„ì ':["ëˆ„ì ","ëª¨ë‘","ì „ë¶€","í•©ê³„"],
    'ë°˜ë³µ':["ë°˜ë³µ","ê¹Œì§€","ë²ˆ"],
    'ì¶œë ¥':["ì¶œë ¥","ë³´ì—¬","í‘œì‹œ","ë‚˜íƒ€","ì¶œë ¥í•´","ì¶œë ¥í•˜"]
}

if __name__ == "__main__":
    csv_path = r'c:\Users\jeayy\Desktop\NLP\find_idea_similarity\DTM(Document-Term Matrix)\coala_ai_response_30013_ML.csv'
    stopwords_set = load_stopwords()
    #word_df = DTM(csv_path,stopwords_set)
    
    idea_df = pd.read_csv(csv_path, encoding='utf-8-sig')
    idea_df = pd.DataFrame(idea_df)
    
    # ìˆ˜ì •: make_idea_morphs ê²°ê³¼ë¥¼ ë‹¤ì‹œ ëŒ€ì…
    idea_df_dict = idea_df.to_dict('records')
    idea_df_dict = make_idea_morphs(idea_df_dict, stopwords_set)
    idea_df_dict = pd.DataFrame(idea_df_dict) 
    
    word_df = pd.read_csv(r'c:\Users\jeayy\Desktop\NLP\find_idea_similarity\DTM(Document-Term Matrix)\30013_word_frequency_table.csv', encoding='utf-8-sig')
    
    idea_df_dict['score'] = 0
    
    # í•„ìˆ˜ ë‹¨ì–´ í™•ì¸ê³¼ í•¨ê»˜ ì ìˆ˜ ê³„ì‚°
    over_score_ideas, qualified_ideas = find_over_threshold_words(
        word_df, idea_df_dict, essential_words, 
        score_threshold=4, frequency_threshold=30
    )
    
    print("\n" + "="*60)
    print("ğŸ¯ ëª¨ë“  í•„ìˆ˜ ë‹¨ì–´ë¥¼ í¬í•¨í•œ ìš°ìˆ˜ ì•„ì´ë””ì–´ë“¤:")
    print("="*60)
    
    if qualified_ideas:
        for result in qualified_ideas:
            print(f"ğŸ“ ì•„ì´ë””ì–´: {result['idea']}")
            print(f"ğŸ“Š ì ìˆ˜: {result['score']}")
            print(f"âœ… í¬í•¨ ë‹¨ì–´: {result['found_categories']}")
            print("-" * 40)
    else:
        print("âŒ ëª¨ë“  í•„ìˆ˜ ë‹¨ì–´ë¥¼ í¬í•¨í•œ ì•„ì´ë””ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("\nğŸ“‹ ë¶€ë¶„ì ìœ¼ë¡œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì•„ì´ë””ì–´ë“¤:")
        for result in over_score_ideas:
            print(f"ì•„ì´ë””ì–´: {result['idea']}")
            print(f"ì ìˆ˜: {result['score']}, ëˆ„ë½: {result['missing_categories']}")
    
    print(f"\nğŸ“ˆ ì „ì²´ ê²°ê³¼: ê¸°ì¤€ ì ìˆ˜ ì´ìƒ {len(over_score_ideas)}ê°œ, ì™„ì „ ì¡°ê±´ ë§Œì¡± {len(qualified_ideas)}ê°œ")